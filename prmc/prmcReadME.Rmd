# Comparison of Price-Rent Ratio Calculation Method

This analysis compares number of methods used to calculate price to rent ratios using a large dataset from Melbourne, Australia.  Unfortunately, our currently licensing on the data does not permit it to be shared.  All code below explained below and held in this repository is free to use, provided you give recognition for doing so. 

The basic order of analysis is to prepared the data, analyze the data, visualize the results and then, finally, assess the predictive quality of the models themselves.  In terms of scripts, the order is as follows:
 
  1. **prmcDataPrep.R**
  2. **prmcDataAnalysis.R**
  3. **prmcDataViz.R**
  4. **prmcPredModels.R**

*(If using raw data from AURIN then data must be built first using the buildAPMData.R file in the root repository)*

\  
&nbsp;

#### Analytical Workflow Diagram

The following diagram shows more details regarding data and overall analytical workflow.

[Diagram](prmcWorkflow.png)

\  
&nbsp;

## Data Preparation

The **prmcDataPrep.R** file handles the data preparation phase of this analysis.  In this context, data preparation refers to the conversion of the raw data (from the source) into the prepared data that is ready for analysis.  

#### Preliminary Commands

The process begin by loading the necessary libraries.

    library(plyr)
    library(dplyr)
    library(reshape2)
    library(stringr)
    library(maptools)
    library(sp)
    library(rgeos)

Next, we source a file containing a set of functions that are used throughout the analysis, **prrFunctions.R**.  This file is sources from it's Github location.  Details of the individual functions contained in this file can be found in the *"Custom Functions"* section at the end of this document. 

     source(paste0('https://raw.githubusercontent.com/andykrause/ausPropMrkt/',
                   'master/prrFunctions.R'))
                   
The final set of preliminary commands set the path to the data as well as the names of the individual files to be loaded.  This analysis requires seven separate files: 1) A files of all sales; 2) a file of rentals; 3) a file contain the space syntax values for all properties (calculated externally in GIS); 4-7) GIS shapefiles of suburb, LGA, SLA1 and post code boundaries. 

    dataPath <- "C:/.../rawData/"
    saleFile <- 'allSales.csv'
    rentFile <- 'allRents.csv'
    ssFile <- 'allSS.csv'
    subGeoFile <- 'Vic_Suburbs.shp'
    lgaGeoFile <- 'Vic_LGAs.shp'
    sla1GeoFile <- 'Vic_SLA1.shp'
    postGeoFile <- 'Vic_PostCodes.shp'
    
### Read in Data

The data is then read into memory using the data path and file names specified above.

     rawSales <- read.csv(paste0(dataPath, saleFile), stringsAsFactors = FALSE)
     rawRents <- read.csv(paste0(dataPath, rentFile), stringsAsFactors = FALSE)
     ssData <- read.csv(paste0(dataPath, ssFile), stringsAsFactors = FALSE)
     subShp <- readShapePoly(paste0(dataPath, subGeoFile))
     lgaShp <- readShapePoly(paste0(dataPath, lgaGeoFile))
     sla1Shp <- readShapePoly(paste0(dataPath, sla1GeoFile))
     postCodeShp <- readShapePoly(paste0(dataPath, postGeoFile))
     
### Data management

The next step is data management.  In this context data management involvees the fixing of data errors and formats, the combination of information from other table and sources, the removal of duplicate observations and the removal of non-essential fields. No observations are removed at this step as these procedures are saved for the *"Data Cleaning"* section that follows.

#### Create unique identifiers

We begin by creating a unique identification number for each sale and rental observation

    rawSales$UID <- paste0('sale', 1:nrow(rawSales))
    rawRents$UID <- paste0('rental', 1:nrow(rawRents))

#### Fix and add fields

Date formats throughout the data are not consistent.  Here we employ the **fixAPMDates()* custom function to standardize all of the data formats.  Again, see the end of the document for explanation of the custom functions.

    rawSales$transDate <- fixAPMDates(rawSales$FinalResultEventDate)
    rawRents$transDate <- fixAPMDates(rawRents$EventDate)

Transaction values for rentals and sales are held in different fields in the raw data. Here they are tranformed into a single, identically names field. 

    rawSales$transValue <- as.numeric(rawSales$FinalResultEventPrice)
    rawRents$transValue <- as.numeric(rawRents$EventPrice)

We then add an identifier as to the type of transaction (prior to combining the two datasets.)

    rawSales$transType <- 'sale'  
    rawRents$transType <- 'rent'

#### Fix latitude and longitude fields

Two separate sets of latitude and longitude values are given, one set for the property itslef and another for the centroid of the street that it fronts.  In a small number of cases no property specific lat/long values are available but street centroid lat/longs are.  To avoid having to filter out the transaction with the missing lat/long values we apply the street centroid value in this small number of cases. 

    sXY <- which(is.na(rawSales$Property_Latitude) | is.na(rawSales$Property_Longitude))
    rXY <- which(is.na(rawRents$Property_Latitude) | is.na(rawRents$Property_Longitude))
    rawSales$Property_Latitude[sXY] <- rawSales$Street_Centroid_Latitude[sXY]
    rawSales$Property_Longitude[sXY] <- rawSales$Street_Centroid_Longitude[sXY]
    rawRents$Property_Latitude[rXY] <- rawRents$Street_Centroid_Latitude[rXY]
    rawRents$Property_Longitude[rXY] <- rawRents$Street_Centroid_Longitude[rXY]

#### Trim fields and combine datasets

Next we remove unnecessary data fields and combine the sales and rental datasets into the **allTrans** object. 

    columnList <- c('UID', 'GeographicalID', 'EventID', 'AddressID', 'FlatNumber', 
                    'transDate', 'transValue', 'transType',
                    'PropertyType', 'Property_Latitude', 'Property_Longitude',
                    'AreaSize', 'Bedrooms', 'Baths', 'Parking','HasFireplace',
                    'HasPool', 'HasGarage', 'HasAirConditioning')
    allTrans <- rbind(rawSales[ ,columnList], rawRents[ ,columnList])

#### Build time specific fields

Here we add a variety of fields dealing with the time of the transaction. First we add the sales year.

    allTrans$transYear <- as.numeric(substr(allTrans$transDate, 1, 4))

Then the month of sale, where June 2010 is equal to month 1

    allTrans$transMonth <- ((12 * (allTrans$transYear - 2010)) + 
                              as.numeric(substr(allTrans$transDate, 6, 7))) - 5
    
Next days, where June 1st 2010 is equal to day 1

    allTrans$transDays <- (as.numeric(allTrans$transDate - as.Date('2010-05-31')))

And, finally, quarter where the third quarter of 2010 is equal to 1 (plus the month of June)

    allTrans$transQtr <- ((allTrans$transMonth - 1) %/% 3) + 1

#### Fix NA in optional fields

For some fields, such as presence of pool or garage, an NA is an acceptable value in that it signifies the lack of the presence of the particular characteristic.  This NA, however, is troublesome in the later analytical steps.  Here we turn these NAs into 0s. 

    naFields <- list('HasPool', 'HasGarage', 'HasAirConditioning', 'HasFireplace')
    for(naF in 1:length(naFields)){
      naX <- which(is.na(allTrans[ ,naFields[[naF]]]))
      allTrans[naX, naFields[[naF]]] <- 0
    }
  
####  Remove duplicates

We check to see if there are duplicate observations, identified here as a property transacting more than once on the same day for the same type of transaction (sale or rental). We create a new field aggregating those three features, remove all observations that are duplicated in that field and then remove the field itself.  

    allTrans$dUID <- paste0(allTrans$AddressID,"..", allTrans$transDate, "..", allTrans$transType)
    allTrans <- subset(allTrans, !duplicated(dUID))
    allTrans$dUID <- NULL  

#### Apply spatial information

Here we apply spatial aggregation information to each observations.  More specifically, using the ``rgeos`` libraries spatial analysis capabilities, we spatially join the LGA, SLA1, Suburb and Postcode location to each observation.  

First we must remove all observations that are missing a latitude and longitude.  Normally an operations requiring the removal of observations (rows) would appear in the data cleaning section that follows, however, due to the necessity of this step prior to assigning spatial information it is undertaken here.

    allTrans <- subset(allTrans, !is.na(Property_Latitude) & !is.na(Property_Longitude))
  
Then we convert to a `SpatialPointsDataFrame`.

    allSP <- SpatialPointsDataFrame(coords=cbind(allTrans$Property_Longitude,
                                               allTrans$Property_Latitude),
                                    data=allTrans)
  
Using the ``over()`` command we then add the postcode designation from the postcode shapefile to the observations.

    spJoin <- over(allSP, postCodeShp)
    allSP@data$postCode <- as.character(spJoin$POA_2006)

Then the suburb, also removing a number of the trailing '- BAL' designations that are found in the shapefile.

    spJoin <- over(allSP, subShp)
    allSP@data$suburb <- as.character(spJoin$NAME_2006)
    allSP@data$suburb <- gsub(' - Bal', '', allSP@data$suburb)
  
Then the SLA1...

    spJoin <- over(allSP, sla1Shp)
    allSP@data$sla1 <- as.character(spJoin$SLA_NAME11)
  
And, finally, the LGA.

    spJoin <- over(allSP, lgaShp)
    allSP@data$lga <- as.character(spJoin$LGA_NAME11)
    
We finish by converting the data back to a non-spatal data frame.    

    allTrans <- allSP@data
    
#### Add space syntax fields

In this step we add the space syntax measurements to the data.  These are measurements of the locations street configurations and overall integration in the larger street network.  Essentially measures of both centrality as well as local network availability.  These metrics are calculated outside of this session in a geographic information system.

The first process here involves trimming out the many possible space syntax metric to one of choice (at the 2500m level) and one of integration (at the 25000m level).

    ssTrim <- ssData[, c('AddressID', 'L_choice_2500', 
                           'T64_Integration_Segment_Length_Wgt_R25000_metric')]
    names(ssTrim)[2:3] <- c('ssChoice', 'ssInteg')

We then attach these value to the transaction observations.

    allTrans$ssChoice <- ssTrim$ssChoice[match(allTrans$AddressID, ssTrim$AddressID)]
    allTrans$ssInteg <- ssTrim$ssInteg[match(allTrans$AddressID, ssTrim$AddressID)]

As temporary objects are large, we clean a number of them from working memory at this juncture.

    rm(rawRents); rm(rawSales); rm(spJoin)
    rm(ssData, ssTrim); gc()

### Data Cleaning

This section deals with the removal of observations from the dataset based on the value in one or more of the field from either the raw data or created/appended above.  

#### Filter by property type

We begin by removing all observation that are not designated as a house or unit (apartment).  This includes terraces, townhomes, villas and duplexes.

    allTrans <- subset(allTrans, PropertyType == 'House' | PropertyType == 'Unit')

#### Missing values

We start by removing all observations with a missing or 0-value in a required field.  Transaction value (sale price or rental amount) is first. 

    allTrans <- subset(allTrans, !is.na(transValue))
    allTrans <- subset(allTrans, transValue  != 0)

Then those observations missing critical home characteristics such as lot size, bedroom or bathroom counts. 

    allTrans <- subset(allTrans, !is.na(AreaSize))
    allTrans <- subset(allTrans, !is.na(Bedrooms))
    allTrans <- subset(allTrans, !is.na(Baths))

And, finally, those missing space syntax values.

    allTrans <- subset(allTrans, !is.na(ssChoice))
    allTrans <- subset(allTrans, !is.na(ssInteg))
  
#### Remove suspect values/outliers
  
In this we remove observation with field values that are either unlikely to be proper values and/or relate to homes that are either extremely small/poor or large/opulent.  These homes do not represent the mass market, trends for which we are interested in estimating.  It is true that these cutoff points are somewhat subjective, however, they have been selected after manually examining the data and choosing points that appropriate either in terms of real world reality (no bathrooms) or natural breaks (such as 8 bedrooms). Note that for bedrooms we have two lower limits.  The first, 0, applies to apartment which could be a studio.  The second, 1, is applied to houses. Also note that there are separate sets of transaction value limits for rentals and sales. Finally, note that rents are quoted by the week (though paid by the month) in Melbourne.   
  
First we set the upper and lower limits. 

    areaLimits <- c(40, 25000)
    bathLimits <- c(1, 8)
    bedLimits <- c(0, 1, 8) 
    rentLimits <- c(125, 2500)
    saleLimits <- c(150000, 4000000)

Then we remove by those limits that apply to all observations (lot size and bathrooms)

    allTrans <- subset(allTrans, AreaSize >= areaLimits[1] & 
                       AreaSize <= areaLimits[2])
    allTrans <- subset(allTrans, Baths >= bathLimits[1] & 
                       Baths <= bathLimits[2])

Next, we remove those units and house that don't meet their respective limits. 

    allTrans <- subset(allTrans, Bedrooms <= bedLimits[3])
    allTransU <- subset(allTrans, PropertyType == 'Unit' & 
                        Bedrooms >= bedLimits[1])
    allTransH <- subset(allTrans, PropertyType == 'House' & 
                        Bedrooms >= bedLimits[2])
    allTrans <- rbind(allTransH, allTransU)

Finally, we split the sales and rentals, apply each transaction value filter and then recombine.

    xSales <- subset(allTrans, transType == 'sale')
    xRentals <- subset(allTrans, transType == 'rent')
    xSales <- subset(xSales, transValue >= saleLimits[1] & 
                       transValue <= saleLimits[2])
    xRentals <- subset(xRentals, transValue >= rentLimits[1] & 
                       transValue <= rentLimits[2])
    allTrans <- rbind(xSales, xRentals)

Again, we clean up the memory

    rm(xSales); rm(xRentals); gc()
    
#### Trim shapefiles to extent of sales

Each of the raw shapefiles for the four geographic aggregations contains all areas for the State of Victoria.  In this step we use the sales and rental observations to limit the shapefiles to the areas covered by the transactional observations. 

First with trim the suburbs

    studySuburbs <- subShp[(which(subShp@data$NAME_2006 %in% 
                             names(table(allTrans$suburb)))), ]
  
Then the post codes

    studyPostCodes <- postCodeShp[(which(postCodeShp@data$POA_2006 %in% 
                                   names(table(allTrans$postCode)))), ]
  
Then the SLA1s

    studySLA1s <- sla1Shp[(which(sla1Shp@data$SLA_NAME11 %in% 
                                  names(table(allTrans$sla1)))), ]
  
And finally the LGAs

    studyLGAs <- lgaShp[(which(lgaShp@data$LGA_NAME11 %in% 
                                  names(table(allTrans$lga)))), ]
  
#### Set space-time limits to geographies

Across all of the future methods used to analyze this data, it is important that sample sizes in each space-time unit of analysis are large enough to limit outlying observations.  In other words, if we are analyzing the median rental yield in a given suburb quarterly that each suburb have at least X number of observation per quarter. These breakdowns also cover the different use-type analyses we intend to do (both uses, house only, unit only and either) We have done this analysis ahead of time and have labeled the transaction such that a field will indicate whether each transaction is valid for each space-time breakdown.  Examples of space-time breakdowns include postcode by year all, suburb by quarter unit only, etc.  A custom function, **prrGeoLimit()** is used to make the calculations.

We begin by making the calculations for all geographies at the year time period

    yearThres <- mapply(prrGeoLimit, 
                        locField=c('postCode', 'sla1', 'suburb', 'lga'), 
                        MoreArgs=list(timeField='transYear',
                                      transData=allTrans,
                                      geoTempLimit=3))
And then add names to the fields

    names(yearThres) <- paste0(rep("YT_"),
                               rep(c('both', 'house', 'unit', 'either'), 4),
                               rep("_", 16),
                               c(rep('postCode',4), rep('sla1',4),
                                 rep('suburb',4), rep('lga', 4)))
  
We then do the same for all geographies quarterly  

    qtrThres <- mapply(prrGeoLimit, 
                       locField=c('postCode', 'sla1', 'suburb', 'lga'), 
                       MoreArgs=list(timeField='transQtr',
                                     transData=allTrans,
                                     geoTempLimit=3))
  
And adding the names

    names(qtrThres) <- paste0(rep("QT_"),
                              rep(c('both', 'house', 'unit', 'either'), 4),
                              rep("_", 16),
                              c(rep('postCode',4), rep('sla1',4),
                              rep('suburb',4), rep('lga', 4))) 
                              
Following this, we use the custom function, **prrApplyThres()** to append the designations to each of the observations.

    allTrans <- prrApplyThres(yearThres[1:4], allTrans, 'YT', 'postCode')
    allTrans <- prrApplyThres(yearThres[5:8], allTrans, 'YT', 'sla1')
    allTrans <- prrApplyThres(yearThres[9:12], allTrans, 'YT', 'suburb')
    allTrans <- prrApplyThres(yearThres[13:16], allTrans, 'YT', 'lga')
    allTrans <- prrApplyThres(qtrThres[1:4], allTrans, 'QT', 'postCode')
    allTrans <- prrApplyThres(qtrThres[5:8], allTrans, 'QT', 'sla1')
    allTrans <- prrApplyThres(qtrThres[9:12], allTrans, 'QT', 'suburb')
    allTrans <- prrApplyThres(qtrThres[13:16], allTrans, 'QT', 'lga')      
    
Then we write the data out as a .csv and the entire working environment as an R workspace.

    save.image(paste0(dataPath, 'cleanData.RData'))
    write.csv(allTrans, paste0(dataPath, 'cleanData.csv'), row.names=F)


\  
&nbsp;



## Data Analysis

The **prmcDataAnalysis.R** file handles the data analysis phase of this analysis.  

#### Preliminary Commands

The process begins by setting a parameter that indicates whether or not to rebuild the data (re-run the **prmcDataPrep.R** file or not)

    reBuildData <- FALSE     

Then we load the necessary libraries.

    library(plyr)
    library(dplyr)
    library(reshape2)
    library(stringr)
    library(maptools)
    library(sp)
    library(rgeos)

Next, we source a file containing a set of functions that are used throughout the analysis, **prrFunctions.R**.  This file is sources from it's Github location.  Details of the individual functions contained in this file can be found in the *"Custom Functions"* section at the end of this document. 

     source(paste0('https://raw.githubusercontent.com/andykrause/ausPropMrkt/',
                   'master/prrFunctions.R'))
                   
We also source an exterior set of function that assist in the data analysis tasks. 

     source(paste0('https://raw.githubusercontent.com/andykrause/',
                   'dataAnalysisTools/master/stShardFunctions.R'))


Finally, we set the path to the data

     dataPath <- "C:/.../rawData/"
     
#### Read in and prepare data

If the **reBuildData** parameter is set to TRUE, then we source the **prmcDataPrep.R** file described above.  If not, then the cleaned data object is loaded into memory.

    if(reBuildData){
       source(paste0('https://raw.githubusercontent.com/andykrause/ausPropMrkt/',
                     'master/akModelComparisonAnalysis/prmcDataPrep.R'))
    } else {
       load(paste0(dataPath, 'cleanData.RData'))
    }
  
### Calculate yield values

In the remainder of this file we calculate the actual yield values for all three methods: 1) Median; 2) Imputation; and 3) Matching.  For the median method, only aggregate values are calculated, for Imputation and Matching first the individual values are computed, then they are aggregated.

#### Median Method

Below we calculate the rental yield values using the median method.  This is accomplished using the **prrStsGeoWrap()** function which is a specialized wrapper function for the more general **spaceTimeShard()** function.  The calculation are done at all five levels and with combined uses and uses (house and unit) separated.

##### Metro Level

    mmMetro <- prrStsGeoWrap(stsData = allTrans,
                             metric=c('transValue', 'transValue'),
                             spaceField='all', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
                              
    mmMetroH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House',],
                              metric=c('transValue', 'transValue'),
                              spaceField='all', timeField='transQtr',
                              defDim='time', stsLimit=3, 
                              calcs=list(median='median'))
                      
    mmMetroU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit',],
                              metric=c('transValue', 'transValue'),
                              spaceField='all', timeField='transQtr',
                              defDim='time', stsLimit=3, 
                              calcs=list(median='median'))
  
##### LGA Level
  
    mmLga<- prrStsGeoWrap(stsData = allTrans,
                          metric=c('transValue', 'transValue'),
                          spaceField='lga', timeField='transQtr',
                          defDim='time', stsLimit=3, 
                          calcs=list(median='median'))
  
    mmLgaH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House', ],
                            metric=c('transValue', 'transValue'),
                            spaceField='lga', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    mmLgaU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit', ],
                            metric=c('transValue', 'transValue'),
                            spaceField='lga', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
##### SLA1 Level
  
    mmSla <- prrStsGeoWrap(stsData = allTrans,
                           metric=c('transValue', 'transValue'),
                           spaceField='sla1', timeField='transQtr',
                           defDim='time', stsLimit=3, 
                           calcs=list(median='median'))
  
    mmSlaH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House', ],
                            metric=c('transValue', 'transValue'),
                            spaceField='sla1', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    mmSlaU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit', ],
                            metric=c('transValue', 'transValue'),
                            spaceField='sla1', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
##### Suburb Level
  
    mmSuburb <- prrStsGeoWrap(stsData = allTrans,
                              metric=c('transValue', 'transValue'),
                              spaceField='suburb', timeField='transQtr',
                              defDim='time', stsLimit=3, 
                              calcs=list(median='median'))
  
    mmSuburbH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House', ],
                               metric=c('transValue', 'transValue'),
                               spaceField='suburb', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))

    mmSuburbU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit', ],
                               metric=c('transValue', 'transValue'),
                               spaceField='suburb', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
##### PostCode Level
  
    mmPostcode <- prrStsGeoWrap(stsData = allTrans,
                                metric=c('transValue', 'transValue'),
                                spaceField='postCode', timeField='transQtr',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median'))
  
    mmPostcodeH <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'House', ],
                                 metric=c('transValue', 'transValue'),
                                 spaceField='postCode', timeField='transQtr',
                                 defDim='time', stsLimit=3, 
                                 calcs=list(median='median'))
    mmPostcodeU <- prrStsGeoWrap(allTrans[allTrans$PropertyType == 'Unit', ],
                                 metric=c('transValue', 'transValue'),
                                 spaceField='postCode', timeField='transQtr',
                                 defDim='time', stsLimit=3, 
                                 calcs=list(median='median'))
  
#### Imputation Method

Next we calculate the rental yield values using the imputation method.  First we specify imputation models for both sales price and rental values.  

    regSpecH <- log(transValue) ~ as.factor(postCode) + as.factor(transQtr) + 
                   log(AreaSize) +Bedrooms + Baths + ssInteg + ssChoice
  
    regSpecU <- log(transValue) ~ as.factor(postCode) + as.factor(transQtr) + 
                   Bedrooms + Baths + ssInteg + ssChoice

We then impute the values using the **prrImputeReg()** function.  This function uses the sales model to impute sales prices for the rental observations and vice versa. Results are calculated separately for houses and units.

    houseResults <- prrImputeReg(regSpecH, 
                            subset(allTrans, transType == 'sale' &
                                             PropertyType == 'House' & 
                                             QT_house_postCode == 1),
                            subset(allTrans, transType == 'rent' &
                                             PropertyType == 'House' & 
                                             QT_house_postCode == 1),
                            verbose=TRUE)
  
    unitResults <- prrImputeReg(regSpecU, 
                           subset(allTrans, transType == 'sale' &
                                    PropertyType == 'Unit' & 
                                    QT_unit_postCode == 1),
                           subset(allTrans, transType == 'rent' &
                                    PropertyType == 'Unit' & 
                                    QT_unit_postCode == 1),
                           verbose=TRUE)

We then extract the imputed and observed values and calculated the rental yield

    irValues <- rbind(houseResults$results, unitResults$results)

    irValues$yield <- (irValues$Rent * 52) / irValues$Price

Finally, we add the yield estimates to the original dataset, removing all those for which a yield could not be calculated

    allTrans$yield <- irValues$yield[match(allTrans$UID, irValues$UID)]
    xTrans <- subset(allTrans, !is.na(yield)) 

The calculations are done at all five levels and with combined uses and uses (house and unit) separated.  The generic **spaceTimeShard()** function is used to calculate the median yield value for each quarter.

##### Metro
  
    irMetro <- spaceTimeShard(stsData = xTrans,
                             metric=c('yield'),
                             spaceField='all', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    irMetroH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                               metric=c('yield'),
                               spaceField='all', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))

    irMetroU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                               metric=c('yield'),
                               spaceField='all', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))

##### LGA
  
    irLga <- spaceTimeShard(stsData = xTrans,
                            metric=c('yield'),
                            spaceField='lga', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    irLgaH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                             metric=c('yield'),
                             spaceField='lga', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    irLgaU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                             metric=c('yield'),
                             spaceField='lga', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median')) 

##### SLA1
  
    irSla <- spaceTimeShard(stsData = xTrans,
                            metric=c('yield'),
                            spaceField='sla1', timeField='transQtr',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    irSlaH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                             metric=c('yield'),
                             spaceField='sla1', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    irSlaU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                             metric=c('yield'),
                             spaceField='sla1', timeField='transQtr',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))

##### Suburb
  
    irSuburb <- spaceTimeShard(stsData = xTrans,
                               metric=c('yield'),
                               spaceField='suburb', timeField='transQtr',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
    irSuburbH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                                metric=c('yield'),
                                spaceField='suburb', timeField='transQtr',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median'))
  
    irSuburbU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                                metric=c('yield'),
                                spaceField='suburb', timeField='transQtr',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median'))
  
##### Post Code
  
    irPostcode <- spaceTimeShard(stsData = xTrans,
                                 metric=c('yield'),
                                 spaceField='postCode', timeField='transQtr',
                                 defDim='time', stsLimit=3, 
                                 calcs=list(median='median'))
  
    irPostcodeH <- spaceTimeShard(xTrans[xTrans$PropertyType == 'House', ],
                                  metric=c('yield'),
                                  spaceField='postCode', timeField='transQtr',
                                  defDim='time', stsLimit=3, 
                                  calcs=list(median='median'))
  
    irPostcodeU <- spaceTimeShard(xTrans[xTrans$PropertyType == 'Unit', ],
                                  metric=c('yield'),
                                  spaceField='postCode', timeField='transQtr',
                                  defDim='time', stsLimit=3, 
                                  calcs=list(median='median'))
  
#### Matching Method

Next we calculate the rental yield values using the matching method.  In the first step we extract all of the matching observations (properties that have both rented and sold) using the **prrSaleRentMatch()** function.

     dmData <- prrSaleRentMatch(sales=allTrans[allTrans$transType=='sale',], 
                                rentals=allTrans[allTrans$transType=='rent',],
                                matchField='AddressID', saleField='transValue',
                                rentField='transValue', timeField='transQtr')


The calculations are then done at all five levels and with combined uses and uses (house and unit) separated.  The generic **spaceTimeShard()** function is used to calculate the median yield value for each quarter.

##### Metro

    dmMetro <- spaceTimeShard(stsData = dmData,
                              metric=c('saleYield'),
                              spaceField='all', timeField='saleTime',
                              defDim='time', stsLimit=3, 
                              calcs=list(median='median'))
  
    dmMetroH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                               metric=c('saleYield'),
                               spaceField='all', timeField='saleTime',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
    dmMetroU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                               metric=c('saleYield'),
                               spaceField='all', timeField='saleTime',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
##### LGA  
  
    dmLga <- spaceTimeShard(stsData = dmData,
                            metric=c('saleYield'),
                            spaceField='lga', timeField='saleTime',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    dmLgaH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                             metric=c('saleYield'),
                             spaceField='lga', timeField='saleTime',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    dmLgaU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                             metric=c('saleYield'),
                             spaceField='lga', timeField='saleTime',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
##### SLA1 
  
    dmSla <- spaceTimeShard(stsData = dmData,
                            metric=c('saleYield'),
                            spaceField='sla1', timeField='saleTime',
                            defDim='time', stsLimit=3, 
                            calcs=list(median='median'))
  
    dmSlaH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                             metric=c('saleYield'),
                             spaceField='sla1', timeField='saleTime',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median'))
  
    dmSlaU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                             metric=c('saleYield'),
                             spaceField='sla1', timeField='saleTime',
                             defDim='time', stsLimit=3, 
                             calcs=list(median='median')) 

##### Suburb
  
    dmSuburb <- spaceTimeShard(stsData = dmData,
                               metric=c('saleYield'),
                               spaceField='suburb', timeField='saleTime',
                               defDim='time', stsLimit=3, 
                               calcs=list(median='median'))
  
    dmSuburbH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                                metric=c('saleYield'),
                                spaceField='suburb', timeField='saleTime',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median'))
  
    dmSuburbU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                                metric=c('saleYield'),
                                spaceField='suburb', timeField='saleTime',
                                defDim='time', stsLimit=3, 
                                calcs=list(median='median')) 
  
##### PostCode 
  
    dmPostcode <- spaceTimeShard(stsData = dmData,
                                 metric=c('saleYield'),
                                 spaceField='postCode', timeField='saleTime',
                                 defDim='time', stsLimit=3, 
                                 calcs=list(median='median'))
  
    dmPostcodeH <- spaceTimeShard(dmData[dmData$PropertyType == 'House',],
                                  metric=c('saleYield'),
                                  spaceField='postCode', timeField='saleTime',
                                  defDim='time', stsLimit=3, 
                                  calcs=list(median='median'))
  
    dmPostcodeU <- spaceTimeShard(dmData[dmData$PropertyType == 'Unit',],
                                  metric=c('saleYield'),
                                  spaceField='postCode', timeField='saleTime',
                                  defDim='time', stsLimit=3, 
                                  calcs=list(median='median')) 
                                  
### Calculate sale price indices

A number of the vizualizations and analyses that follow require a base price index. In this section we extract the coefficients from the hedonic price models to create use specific as well as a combined price index.
                                  
We begin by extracting the coefficients from the hedonic price models developed in the imputation method

    houseModel <- as.data.frame(houseResults$saleModel$coef)
    unitModel <- as.data.frame(unitResults$saleModel$coef)

    houseTimeCoefs <- c(0, houseModel$Estimate[grep('transQtr', 
                                                    rownames(houseModel))])
    unitTimeCoefs <- c(0, unitModel$Estimate[grep('transQtr', 
                                                    rownames(unitModel))])
  
We then convert these into an index of change, where the first period is the base.  This is done for houses and unit separately.
  
    houseIndex <- c(0, (houseTimeCoefs[-1] - 
                          houseTimeCoefs[-length(houseTimeCoefs)]))
    unitIndex <- c(0, (unitTimeCoefs[-1] - 
                         unitTimeCoefs[-length(unitTimeCoefs)]))
  
We then create a combined index by averaging the two use-specific indices.  
  
    allIndex <- (houseIndex + unitIndex) / 2
  
Finally, we convert this into a list object for flexible use later.

    indexList <- list(all = allIndex,
                      house = houseIndex,
                      unit = unitIndex)
                                    
### Combine results

Before moving on to visualization of the result we first combine all results at the geographic level.  In addition to combining the results into a list object, we also create an aggregated set of results where all three method are combined into singular data.frame object for each of computation (using the **prrAggrGeoData()** function).  This includes the price indices developed above. 

    metroList <- list(mm=list(all=mmMetro, house=mmMetroH, unit=mmMetroU),
                      ir=list(all=irMetro, house=irMetroH, unit=irMetroU),
                      dm=list(all=dmMetro, house=dmMetroH, unit=dmMetroU))
    metroData <- prrAggrGeoData(metroList, indexList)


    lgaList <- list(mm=list(all=mmLga, house=mmLgaH, unit=mmLgaU),
                    ir=list(all=irLga, house=irLgaH, unit=irLgaU),
                    dm=list(all=dmLga, house=dmLgaH, unit=dmLgaU))
    lgaData <- prrAggrGeoData(lgaList, indexList, geoSplit=TRUE)


    slaList <- list(mm=list(all=mmSla, house=mmSlaH, unit=mmSlaU),
                    ir=list(all=irSla, house=irSlaH, unit=irSlaU),
                    dm=list(all=dmSla, house=dmSlaH, unit=dmSlaU))
    slaData <- prrAggrGeoData(slaList, indexList, geoSplit=TRUE)


    postcodeList <- list(mm=list(all=mmPostcode, house=mmPostcodeH, unit=mmPostcodeU),
                         ir=list(all=irPostcode, house=irPostcodeH, unit=irPostcodeU),
                         dm=list(all=dmPostcode, house=dmPostcodeH, unit=dmPostcodeU))
    postcodeData <- prrAggrGeoData(postcodeList, indexList, geoSplit=TRUE)


    suburbList <- list(mm=list(all=mmSuburb, house=mmSuburbH, unit=mmSuburbU),
                       ir=list(all=irSuburb, house=irSuburbH, unit=irSuburbU),
                       dm=list(all=dmSuburb, house=dmSuburbH, unit=dmSuburbU))
      suburbData <- prrAggrGeoData(suburbList, indexList, geoSplit=TRUE)
  
Finally, we save only the necessary objects for use in subsequent analyses.

    save(metroList, metroData, lgaList, lgaData, slaList, slaData,
         postcodeList, postcodeData, suburbList, suburbData, dmData,
         file=paste0(dataPath, 'analysisResults.RData'))
         
\  
&nbsp;

## Data Visualization

The **prmcDataViz.R** file handles the visualization of the results.  

#### Preliminary Commands

We begin by loading the necessary libraries.

    library(plyr)
    library(dplyr)
    library(ggplot2)
    library(reshape2)
    library(stringr)
    library(maptools)
    library(sp)
    library(rgeos)
    library(grid)
    
Next, we set the paths to the data analysis object and to a number of shapefiles for creating maps.  Shapefiles include boundary files for the four different geographic aggregations being analyzed -- suburb, LGA, SLA1 and post code.

    dataPath <- "C:/.../rawData/"
    subGeoFile <- 'Vic_Suburbs.shp'
    lgaGeoFile <- 'Vic_LGAs.shp'
    sla1GeoFile <- 'Vic_SLA1.shp'
    postGeoFile <- 'Vic_PostCodes.shp'

We then set two custom plotting theme for our ggplots; one with a black background for presentation and another with white for publication.

     theme_black <- theme_grey() +
      theme(text = element_text(size=9),
          panel.background = element_rect(colour='black', fill='black'),
          panel.grid.major=element_line(colour='gray20'),
          panel.grid.minor=element_line(colour='gray20'),
          plot.background=element_rect(fill='gray10'),
          axis.title.y=element_text(colour='white'),
          axis.text.y=element_text(hjust=1),
          legend.position='bottom',
          legend.background=element_rect(fill='gray10'),
          legend.key=element_rect(fill='gray10', color='gray10'),
          legend.text=element_text(color='white'),
          legend.title=element_blank())
  

    theme_prr <- theme_grey() +
      theme(text = element_text(size=11),
          panel.background = element_rect(colour='gray95', fill='gray95'),
          panel.grid.major=element_line(colour='white', size=.5),
          panel.grid.minor=element_line(colour='white', size=.1),
          plot.background=element_rect(fill='white'),
          axis.title.y=element_text(colour='black'),
          axis.text.y=element_text(hjust=1),
          legend.position='bottom',
          legend.background=element_rect(fill='white'),
          legend.key=element_rect(fill='white', color='white'),
          legend.text=element_text(color='black'),
          legend.title=element_blank(),
          legend.key.width=unit(2, "cm"),
          strip.background = element_rect(fill = "orange", 
                                          color = "orange", size = .1),
          strip.text.x = element_text(face = "bold"),
          strip.text.y = element_text(face = "bold"))
          
#### Load Data   

In this section we load the necessary data for visualization.  We start by loading in the saved R objects from the data analysis stage.

    load(paste0(dataPath, 'analysisResults.RData'))

Next, the four shapefiles are loaded.

    subShp <- readShapePoly(paste0(dataPath, subGeoFile))
    lgaShp <- readShapePoly(paste0(dataPath, lgaGeoFile))
    sla1Shp <- readShapePoly(paste0(dataPath, sla1GeoFile))
    postCodeShp <- readShapePoly(paste0(dataPath, postGeoFile))
         
#### Data Visualization

Here we begin the data visualization.  We start by setting the three colors (grayscale appropriate), sizes and line types that we'll use in the plotting of the three different methods.

    methCols <- c('navy', 'royalblue2', 'skyblue')
    methSizes <- c(.5, 1.25, 2)
    methLines <- c(1, 1, 1)  

##### Metro Scale

We start with plots at the metro scale. Or, in other words, plots showing analysis of data for the aggregated metropolitan area.

First, a comparison of the three methods with houses and units combined.

    metroPlot <- ggplot(metroData$mix$comp,
                      aes(x=timeName, y=yield, group=method)) + 
               geom_line(aes(colour=method, size=method, linetype=method,
                             lineend='round', linejoin='round')) +
               scale_size_manual(values=methSizes) +
               scale_colour_manual(values=methCols) +
               scale_linetype_manual(values=methLines) + 
               xlab("") + ylab("Rental Yield\n") +
               scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
               scale_y_continuous(limits=c(.032, .048),
                                  breaks=seq(.032, .048, .002), 
                                  labels=paste0(format(100 * (seq(.032,
                                                                  .048, .002)),
                                          nsmall=1), "%")) +
               theme_prr

Next we plot the same, but with the results weighted by use

    metroPlotUW <- ggplot(metroData$useWgt$comp, 
                        aes(x=timeName, y=yield, group=method)) + 
                 geom_line(aes(colour=method, size=method, linetype=method,
                               lineend='round', linejoin='round')) +
                 scale_size_manual(values=methSizes) +
                 scale_colour_manual(values=methCols) +
                 scale_linetype_manual(values=methLines) + 
                 xlab("") + ylab("Rental Yield\n") +
                 scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
                 scale_y_continuous(limits=c(.032, .048),
                                    breaks=seq(.032, .048, .002), 
                                    labels=paste0(format(100 * 
                                                           (seq(.032, .048,
                                                                .002)),
                                            nsmall=1), "%")) +
                 theme_prr
  
We then separate the analysis by use (house vs. unit).

    metroPlotUse <- ggplot(metroData$use$comp,
                         aes(x=timeName, y=yield, group=method)) + 
                  geom_line(aes(colour=method, size=method, linetype=method,
                                lineend='round', linejoin='round')) +
                  scale_size_manual(values=methSizes) +
                  scale_colour_manual(values=methCols) +
                  scale_linetype_manual(values=methLines) + 
                  xlab("") + ylab("Rental Yield\n") +
                  scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
                  scale_y_continuous(limits=c(.030, .050),
                                     breaks=seq(.030, .050, .002), 
                                     labels=paste0(format(100 * (
                                       seq(.030, .050, .002)),
                                            nsmall=1), "%")) +
                  facet_wrap(~use) +
                  theme_prr
  
Next, a plot showing the differences between the three methods against the appreciation rate in the quarter.  This plot shows the differences between the combined house and unit analysis.

    metroDiffPlot_A <- ggplot(metroData$mix$diff, aes(x=pIndex, y=dif)) + 
                     geom_point(colour='black', size=2) + 
                     geom_smooth(method=lm) +
                     xlab("Home Price Movement in Qtr") +
                     ylab("Difference in Rental Yield Estimate\n") +
                     scale_x_continuous(limits=c(-.03, .06),
                                        breaks=seq(-.02, .06, .02), 
                                        labels=paste0(format(100 *
                                                      (seq(-.02, .06, .02)),
                                                       nsmall=1), "%")) +
                     scale_y_continuous(limits=c(0, .011),
                                        breaks=seq(0, .01, .002), 
                                        labels=paste0(format(100 * 
                                                      (seq(0, .01, .002)),
                                                       nsmall=1), "%")) +
                     facet_wrap(~method) +
                     theme_prr

Next, a plot showing the differences between the three methods against the time.  This plot shows the differences between the combined house and unit analysis.

    metroDiffPlot_T <- ggplot(metroData$mix$diff, aes(x=timeName, y=dif)) + 
                            geom_point(colour='black', size=2) + 
                     geom_smooth(method=lm) +
                     xlab("") +
                     ylab("Difference in Rental Yield Estimate\n") +
                     scale_x_continuous(breaks=seq(2, 18, 4), 
                                        labels=2011:2015) +
                     scale_y_continuous(limits=c(0, .011),
                                        breaks=seq(0, .01, .002), 
                                        labels=paste0(format(100 * 
                                                     (seq(0, .01, .002)),
                                                      nsmall=1), "%")) +
                     facet_wrap(~method) +
                     theme_prr
  
A plot showing the differences between the three methods against the appreciation rate in the quarter.  This plot shows the differences between the weigthed house and unit analysis.

    metroDiffUWPlot_A <- ggplot(metroData$useWgt$diff, aes(x=pIndex, y=dif)) + 
                       geom_point(colour='black', size=2) + 
                       geom_smooth(method=lm) +
                       xlab("Home Price Movement in Qtr") +
                       ylab("Difference in Rental Yield Estimate\n") +
                       scale_x_continuous(limits=c(-.03, .06),
                                          breaks=seq(-.02, .06, .02), 
                                          labels=paste0(format(100 *
                                                       (seq(-.02, .06, .02)),
                                                        nsmall=1), "%")) +
                       scale_y_continuous(limits=c(0, .011),
                                          breaks=seq(0, .01, .002), 
                                          labels=paste0(format(100 * 
                                                       (seq(0, .01, .002)),
                                                        nsmall=1), "%")) +
                       facet_wrap(~method) +
                       theme_prr
  
A plot showing the differences between the three methods against time.  This plot shows the differences between the weighted house and unit analysis.

    metroDiffUWPlot_T <- ggplot(metroData$useWgt$diff, aes(x=timeName, y=dif)) + 
                       geom_point(colour='black', size=2) + 
                       geom_smooth(method=lm) +
                       xlab("") +
                       ylab("Difference in Rental Yield Estimate\n") +
                       scale_x_continuous(breaks=seq(2, 18, 4), 
                                          labels=2011:2015) +
                       scale_y_continuous(limits=c(0, .011),
                                          breaks=seq(0, .01, .002), 
                                          labels=paste0(format(100 * 
                                                       (seq(0, .01, .002)),
                                                        nsmall=1), "%")) +
                       facet_wrap(~method) +
                       theme_prr
  
A plot showing the differences between the three methods against the appreciation rate in the quarter.  This plot shows the differences between the separated house and unit analysis.

    metroDiffUPlot_A <- ggplot(metroData$use$diff, aes(x=pIndex, y=dif)) + 
                      geom_point(colour='black', size=2) + 
                      geom_smooth(method=lm) +
                      xlab("Home Price Movement in Qtr") +
                      ylab("Difference in Rental Yield Estimate\n") +
                      scale_x_continuous(limits=c(-.03, .07),
                                         breaks=seq(-.02, .06, .02), 
                                         labels=paste0(format(100 *
                                                      (seq(-.02, .06, .02)),
                                                       nsmall=1), "%")) +
                      scale_y_continuous(limits=c(0, .0098),
                                         breaks=seq(0, .008, .002), 
                                         labels=paste0(format(100 * 
                                                      (seq(0, .008, .002)),
                                                       nsmall=1), "%")) +
                      facet_wrap(~use+method) +
                      theme_prr
                      
A plot showing the differences between the three methods against time.  This plot shows the differences between the separated house and unit analysis.

    metroDiffUPlot_T <- ggplot(metroData$use$diff, aes(x=timeName, y=dif)) + 
                      geom_point(colour='black', size=2) + 
                      geom_smooth(method=lm) +
                      xlab("") + 
                      ylab("Difference in Rental Yield Estimate\n")  +
                      scale_x_continuous(breaks=seq(2, 18, 4), 
                                         labels=2011:2015) +
                      scale_y_continuous(limits=c(-.001, .011),
                                         breaks=seq(0, .01, .002), 
                                         labels=paste0(format(100 * 
                                                      (seq(0, .01, .002)),
                                                       nsmall=1), "%")) +
                      facet_wrap(~use+method) +
                      theme_prr

This plot is an aggregate comparison of all difference between the three methods -- combined, use weighted and use separated.

     metroDiffComp <- ggplot(metroData$mix$diff, aes(x=timeName, y=dif)) + 
                      geom_point(colour='black', size=0) + 
                      geom_smooth(method=lm, se=FALSE, colour='black',
                                  size=1) +
                      geom_smooth(data=metroData$useWgt$diff,
                                  aes(x=timeName, y=dif),
                                  method=lm, se=FALSE,
                                  colour='red', size=1) +
                      geom_smooth(data=metroData$use$diff[1:60,],
                                  aes(x=timeName, y=dif),
                                  method=lm, se=FALSE,
                                  colour='blue', size=1) +
                      geom_smooth(data=metroData$use$diff[61:120,],
                                  aes(x=timeName, y=dif),
                                  method=lm, se=FALSE,
                                  colour='navy', size=1) +
                      xlab("") + 
                      ylab("Difference in Rental Yield Estimate\n")  +
                      scale_x_continuous(breaks=seq(2, 18, 4), 
                                         labels=2011:2015) +
                      scale_y_continuous(limits=c(-.001, .011),
                                         breaks=seq(0, .01, .002), 
                                         labels=paste0(format(100 * 
                                                      (seq(0, .01, .002)),
                                                       nsmall=1), "%")) +
                      facet_wrap(~method) +
                      theme_prr
    
We then build plot showing the differences between the methods at the LGA level.  This plot shows the comparisons of the combined house and unit analysis.

    lgaPlot <- ggplot(lgaData$mix$comp, 
                    aes(x=timeName, y=yield, group=spaceName)) + 
    geom_line(colour='gray50', size= 0.1, lineend='round', linejoin='round') +
    geom_line(data=lgaData$mixWgt$comp, 
              aes(x=timeName, y=yield),
              colour='black', size=1.5, lineend='round', linejoin='round') +
    xlab("") + ylab("Rental Yield\n") +
    scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
    scale_y_continuous(limits=c(.015, .059),
                       breaks=seq(.015, .059, .004), 
                       labels=paste0(format(100 * (seq(.015, .059, .004)),
                                            nsmall=1), "%")) +
    facet_wrap(~method)+
    theme_prr
  
We then plot the differences versus time (again, combined house and unit analysis) at the LGA level.
  
    lgaDiffComp_T <- ggplot(lgaData$mix$diff, aes(x=timeName, y=dif, 
                                              group=spaceName)) + 
                 geom_point(colour='black', size=0) + 
                 stat_smooth(data=lgaData$mix$diff,
                             aes(x=timeName, y=dif, group=spaceName),
                             method=loess, se=FALSE, colour='gray50') +
    stat_smooth(data=lgaData$mixWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=TRUE,
                colour='black', size=2) +
    stat_smooth(data=lgaData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=TRUE,
                colour='blue', size=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    facet_wrap(~method) +
    theme_prr


We then plot the differences versus quarterly appreciation (again, combined house and unit analysis) at the LGA level.

    lgaDiffComp_A <- ggplot(lgaData$mix$diff, aes(x=pIndex, y=dif, 
                                                group=spaceName)) + 
    geom_point(colour='black', size=0) + 
    stat_smooth(data=lgaData$mix$diff,
                aes(x=pIndex, y=dif, group=spaceName),
                method=loess, se=FALSE, colour='gray50') +
    stat_smooth(data=lgaData$mixWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=TRUE,
                colour='black', size=2) +
    stat_smooth(data=lgaData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=TRUE,
                colour='blue', size=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    facet_wrap(~method) +
    theme_prr
  
We then build plot showing the differences between the methods at the suburb level.  This plot shows the comparisons of the combined house and unit analysis.

    suburbPlot <- ggplot(suburbData$mix$comp, aes(x=timeName, y=yield, 
                                          group=spaceName)) + 
    geom_line(colour='gray50', size= 0.1, lineend='round', linejoin='round') +
    geom_line(data=suburbData$mixWgt$comp, 
              aes(x=timeName, y=yield),
              colour='black', size=1.5, lineend='round', linejoin='round') +
    xlab("") + ylab("Rental Yield\n") +
    scale_x_continuous(breaks=seq(2, 18, 4), labels=2011:2015) +
    scale_y_continuous(limits=c(.01, .079),
                       breaks=seq(.015, .059, .004), 
                       labels=paste0(format(100 * (seq(.015,
                                                       .059, .004)),
                                            nsmall=1), "%")) +
    facet_wrap(~method)+
    theme_prr
  
We then plot the differences versus time (again, combined house and unit analysis) at the suburb level.

    suburbDiffComp_T <- ggplot(suburbData$mix$diff, aes(x=timeName, y=dif, 
                                              group=spaceName)) + 
    geom_point(colour='black', size=0) + 
    stat_smooth(data=suburbData$mix$diff,
                aes(x=timeName, y=dif, group=spaceName),
                method=loess, se=FALSE, colour='gray50') +
    stat_smooth(data=suburbData$mixWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2) +
    stat_smooth(data=suburbData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    scale_y_continuous(limits=c(-.001, .011),
                       breaks=seq(0, .01, .002), 
                       labels=paste0(format(100 * (seq(0, .01, .002)),
                                                 nsmall=1), "%")) +
    facet_wrap(~method) +
    theme_prr
  

We then plot the differences versus quarterly appreciation (again, combined house and unit analysis) at the suburb level.

     suburbDiffComp_A <- ggplot(suburbData$mix$diff, 
                             aes(x=pIndex, y=dif, group=spaceName)) + 
    geom_point(colour='black', size=0) + 
    stat_smooth(data=suburbData$mix$diff,
                aes(x=pIndex, y=dif, group=spaceName),
                method=loess, se=FALSE, colour='gray50') +
    stat_smooth(data=suburbData$mixWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2) +
    stat_smooth(data=suburbData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    scale_y_continuous(limits=c(-.001, .011),
                       breaks=seq(0, .01, .002), 
                       labels=paste0(format(100 * (seq(0, .01, .002)),
                                            nsmall=1), "%")) +
    facet_wrap(~method) +
    theme_prr
  
Finally, we plot all differnces against time for the metro, LGA and suburb analyses.

    allDiffComp_T <- ggplot(lgaData$mix$diff, aes(x=timeName, y=dif, 
                                                    group=spaceName)) + 
                 geom_point(colour='black', size=0) + 
    stat_smooth(data=suburbData$mixWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2, linetype=2) +
    stat_smooth(data=lgaData$mixWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2) +
    stat_smooth(data=lgaData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2) +
    stat_smooth(data=metroData$mix$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='red', size=2) +
    stat_smooth(data=metroData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='red', size=2, linetype=2) +
    stat_smooth(data=suburbData$useWgt$diff,
                aes(x=timeName, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2, linetype=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    scale_y_continuous(limits=c(-.001, .011),
                       breaks=seq(0, .01, .002), 
                       labels=paste0(format(100 * 
                                              (seq(0, .01, .002)),
                                            nsmall=1), "%")) +
    facet_wrap(~method) +
    theme_prr
 
Finally, we plot all differences against quarterly appreciation for the metro, LGA and suburb analyses.

    allDiffComp_A <- ggplot(lgaData$mix$diff, aes(x=pIndex, y=dif, 
                                                group=spaceName)) + 
    geom_point(colour='black', size=0) + 
    stat_smooth(data=suburbData$mixWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2) +
    stat_smooth(data=lgaData$mixWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='black', size=2, linetype=2) +
    stat_smooth(data=lgaData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2) +
    stat_smooth(data=metroData$mix$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='red', size=2) +
    stat_smooth(data=metroData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='red', size=2, linetype=2) +
    stat_smooth(data=suburbData$useWgt$diff,
                aes(x=pIndex, y=dif),
                method=loess, se=FALSE,
                colour='blue', size=2, linetype=2) +
    xlab("") + 
    ylab("Difference in Rental Yield Estimate\n")  +
    scale_y_continuous(limits=c(0, .0105),
                       breaks=seq(0, .008, .002), 
                       labels=paste0(format(100 * (seq(0, .008, .002)),
                                                nsmall=1), "%")) +
    facet_wrap(~method) +
    theme_prr
    
\  
&nbsp;

## Predictive Modeling

The **prmcPredModels.R** file contains code where we test the predictive ability of the three methods across the five different spatial aggregation levels.

#### Preliminary Commands 

Per the usual, we begin by loading the necessary libraries.

    library(plyr)
    library(dplyr)
    library(ggplot2)
    library(reshape2)
    library(stringr)
    library(maptools)
    library(sp)
    library(rgeos)
    library(grid)

We then source the necessary custom functions (directly from Github)

    source(paste0('https://raw.githubusercontent.com/andykrause/ausPropMrkt/',
                  'master/prrFunctions.R'))
  
#### Load Data   

Next we load the R objects from the data analysis phase.  First we set the path to the data.

    dataPath <- "C:/.../rawData/"

Then load the objects.

    load(paste0(dataPath, 'analysisResults.RData'))
    
#### Extract necessary data 
  
Next we convert the analytical data objects into data objects that are useful for predictive modeling.  To do so, we use the **prrGetYields()** custom function.

    metroYields <- prrGetYields(metroData)
    lgaYields <- prrGetYields(lgaData)
    slaYields <- prrGetYields(slaData)
    postcodeYields <- prrGetYields(postcodeData)
    suburbYields <- prrGetYields(suburbData)

We also add a unique identifier to the direct match data at this step

    dmData$uID <- 1:nrow(dmData)

#### Estimate prediction errors 
  
We begin by estimating the raw prediction errors, where prediction error is the difference between the actual price to rent ratio for each of the matched observations versus that predicted by the three estimation methods.  We use the custom function **prrPredModelWrap** to complete this operation.

Starting with the metro level, combined and use separated (*byUse=TRUE*)

    metroRes <- prrPredModelWrap(dmData, metroYields)
    metroResU <- prrPredModelWrap(dmData, metroYields, byUse=TRUE)
  
Then at the LGA level

    lgaRes <- prrPredModelWrap(dmData, lgaYields, byGeog=TRUE, geoField='lga')
    lgaResU <- prrPredModelWrap(dmData, lgaYields, byGeog=TRUE, 
                                geoField='lga', byUse=TRUE)
  
And, then at the suburb level

    suburbRes <- prrPredModelWrap(dmData, suburbYields, byGeog=TRUE, 
                                  geoField='suburb')
    suburbResU <- prrPredModelWrap(dmData, suburbYields, byGeog=TRUE, 
                                   geoField='suburb', byUse=TRUE)


#### Attached Prediction Errors
                                   
Next, we attach the individual prediction errors from each of the three methods and the two use categories to the individual observations in the matched dataset.  This is done for all three levels of analysis.

At the metro level

    dmData$mMed <- metroRes$median$error[match(dmData$uID, metroRes$median$uID)]
    dmData$mMedU <- metroResU$median$error[match(dmData$uID, metroResU$median$uID)]
    dmData$mImp <- metroRes$impute$error[match(dmData$uID, metroRes$impute$uID)]
    dmData$mImpU <- metroResU$impute$error[match(dmData$uID, metroResU$impute$uID)]
    dmData$mMat <- metroRes$match$error[match(dmData$uID, metroRes$match$uID)]
    dmData$mMatU <- metroResU$match$error[match(dmData$uID, metroResU$match$uID)]

The LGA level

    dmData$lMed <- lgaRes$median$error[match(dmData$uID, lgaRes$median$uID)]
    dmData$lMedU <- lgaResU$median$error[match(dmData$uID, lgaResU$median$uID)]
    dmData$lImp <- lgaRes$impute$error[match(dmData$uID, lgaRes$impute$uID)]
    dmData$lImpU <- lgaResU$impute$error[match(dmData$uID, lgaResU$impute$uID)]
    dmData$lMat <- lgaRes$match$error[match(dmData$uID, lgaRes$match$uID)]
    dmData$lMatU <- lgaResU$match$error[match(dmData$uID, lgaResU$match$uID)]

The suburb level

    dmData$sMed <- suburbRes$median$error[match(dmData$uID, suburbRes$median$uID)]
    dmData$sMedU <- suburbResU$median$error[match(dmData$uID, suburbResU$median$uID)]
    dmData$sImp <- suburbRes$impute$error[match(dmData$uID, suburbRes$impute$uID)]
    dmData$sImpU <- suburbResU$impute$error[match(dmData$uID, suburbResU$impute$uID)]
    dmData$sMat <- suburbRes$match$error[match(dmData$uID, suburbRes$match$uID)]
    dmData$sMatU <- suburbResU$match$error[match(dmData$uID, suburbResU$match$uID)]

#### Calculate Prediction Error

Before calculation, we first create an object contain the absolute value of the individual prediction errors.

    absPredResults <- lapply(dmData[ ,(which(colnames(dmData) == 'mMed'):
                                        which(colnames(dmData) == 'sMatU'))], abs)

We then calculate the median of each of the 18 predictions (3 methods x 2 use types x 3 geographic levels)

    absPredMed <- lapply(absPredResults, median, na.rm=TRUE)

The results are then placed in a 6 x 3 table.

    absPredMed <- as.matrix(unlist(absPredMed))
    predTable <- data.frame(metro=absPredMed[1:6],
                            lga=absPredMed[7:12],
                            suburb=absPredMed[13:18])
    rownames(predTable) <- c('Median', 'Median by Use', 'Impute', 'Impute by Use',
                             'Match', 'Match by Use')

#### Calculate Hit Rate

We also calculate the hit rate, or the percentage of total observations that each methods is able to give a predicted value on.  

First we build a simple function to count the number of non-predicted observations in each method.

    countNA <- function(x){length(which(is.na(x)))/length(x)}

We then calculate the hit rate for each of the 18 predictions

    hitRate <- lapply(absPredResults, countNA)

The results are then placed in a 6 x 3 table

    hitRate <- 1 - as.matrix(unlist(hitRate))
    hrTable <- data.frame(metro=hitRate[1:6],
                          lga=hitRate[7:12],
                          suburb=hitRate[13:18])
    rownames(hrTable) <- c('Median', 'Median by Use', 'Impute', 'Impute by Use',
                           'Match', 'Match by Use')

#### Save workspace 

Finally we save the workspace.

    save(dmData, predTable, hrTable, 
         file=paste0(dataPath, 'predModelResults.RData'))
         
    
\  
&nbsp;
\  
&nbsp;
\  
&nbsp;
\  
&nbsp;

## Custom Functions ----------------------------------------------------------------------

Here we describe in detail the functionality of all of the custom functions used in this analysis. the code for these functions is found in the **prrFunctions.R** file.

### fixAPMDates()

This function converts the various date formats in the APM data to the standard R format.

This function takes 1 argument:

1. **xDates**: A vector of incorrectly formatted date objects from APM

Declare the function

    fixAPMDates <- function(xDates){
  
Load required libraries

    require(stringr)

Remove any time stamps from the dates

    xDates <- gsub(" 0:00", "", xDates)

Find the character location of the date separators (slashes)
  
    sLoc <- matrix(unlist(str_locate_all(xDates, '/')), ncol=4, byrow=TRUE)[,1:2]
  
Extract and correct the days component (if it is missing a leading 0)

    days <- as.numeric(substr(xDates, 1, sLoc[ ,1] - 1))
    days <- ifelse(days < 10, paste0('0', days), as.character(days))
    
Extract and correct the months component (if it is missing a leading 0)

    months <- as.numeric(substr(xDates, sLoc[ ,1] + 1, sLoc[ ,2] - 1))
    months <- ifelse(months < 10, paste0('0', months), as.character(months))
  
Extract and correct the years component (if it is missing a leading 20)

    years <- as.numeric(substr(xDates, sLoc[ ,2] + 1, 50))
    years <- ifelse(years < 2000, paste0('20', years), as.character(years))

Recombine the three components into the R date format

    newDates <- as.Date(paste0(days, '/' , months, '/', years), "%d/%m/%Y")

Return value to the function call

    return(newDates)
    }

\  
&nbsp;

### prrImputeReg()

This function uses linear regression to create imputed rent and sales values.

This function takes 4 arguments:

1. **formula**: The linear regression formula used to create rent and price estimates
2. **saleData**: A data frame of sales observations
3. **rentData**: A data frame of rental observations
4. **verbose**: (FALSE)  Should progress be reported to the console?

Declare the function

    prrImputeReg <- function(formula, saleData, rentData, verbose = FALSE){
  
Fit the sale and rent models

    if(verbose) cat('Estimating sale and rent models\n')
    saleModel <- lm(formula, data=saleData)
    rentModel <- lm(formula, data=rentData)

Use model to predict rents for sale observations and vice versa

    if(verbose) cat('Imputing values\n')
    impPrice <- exp(predict(saleModel, newdata=rentData))
    impRent <- exp(predict(rentModel, newdata=saleData))
  
Add the imputed values to the original data frames

    if(verbose) cat('Stacking observed and imputed values\n')
    saleData$Price <- saleData$transValue
    rentData$Price <- impPrice
    saleData$Rent <- impRent
    rentData$Rent <- rentData$transValue
  
Merge the sales and rent data into a single data frame

    if(verbose) cat('Merging data\n')
    allData <- rbind(saleData, rentData)

Extract the model coefficient, diagnostics and residuals.  Place in a list object for the rent and the sales model.

    saleModelInfo <- list(coef=summary(saleModel)$coefficients,
                        r2=summary(saleModel)$r.squared,
                        sigma=summary(saleModel)$r.squared,
                        resid=summary(saleModel)$residuals)
    rentModelInfo <- list(coef=summary(rentModel)$coefficients,
                        r2=summary(rentModel)$r.squared,
                        sigma=summary(rentModel)$r.squared,
                        resid=summary(rentModel)$residuals)
  
Return the results in a list object.  First the observation ID with Price and Rent, then the two list of model information.  
  
    return(list(results = allData[ ,c('UID', 'Price', 'Rent')],
                saleModel = saleModelInfo,
                rentModel = rentModelInfo))
         
    }

\  
&nbsp;

### prrGeoLimit()

This function takes a set of transactional data and a geographic field and determines which geographic areas have enough observations in them per the selected time breakdown.

This function takes 4 arguments:

1. **transData**: The data frame of the transactions
2. **locField**: ('locName') The name of the field with the location in it. 
3. **timeField**: ('transYear') The name of the field with the temporal indicator in it
4. **geoTempLimit**: (3) How many observations are required in each location at each time

Declare the function

    prrGeoLimit <- function(transData, locField = 'locName', timeField = 'transYear', 
                            geoTempLimit = 3){  
  
Split the transaction by use and by transaction type

    houseSales <- subset(transData, PropertyType == 'House' &
                           transType == 'sale')
    unitSales <- subset(transData, PropertyType == 'Unit' & 
                          transType == 'sale')
    houseRentals <- subset(transData, PropertyType == 'House' & 
                             transType == 'rent')
    unitRentals <- subset(transData, PropertyType == 'Unit' & 
                            transType == 'rent')
  
Determine which areas meet the criteria for each of the use/transaction type permutations.  This process creates a list of geographic indicators that meet the criteria in the **xxGeo** objects.

House Sales

    saleHTable <- table(houseSales[,locField], houseSales[,timeField])
    shKeep <- which(apply(saleHTable, 1, min) >= geoTempLimit)
    shGeo <- rownames(saleHTable[shKeep, ])

Unit Sales

    saleUTable <- table(unitSales[,locField], unitSales[,timeField])
    suKeep <- which(apply(saleUTable, 1, min) >= geoTempLimit)
    suGeo <- rownames(saleUTable[suKeep, ])

House Rentals

    rentHTable <- table(houseRentals[,locField], houseRentals[,timeField])
    rhKeep <- which(apply(rentHTable, 1, min) >= geoTempLimit)
    rhGeo <- rownames(rentHTable[rhKeep, ])

Unit Rentals

    rentUTable <- table(unitRentals[,locField], unitRentals[,timeField])
    ruKeep <- which(apply(rentUTable, 1, min) >= geoTempLimit)
    ruGeo <- rownames(rentUTable[ruKeep, ])
  
Intersect all acceptable lists of geographies to find those that meet the criteria in all four cases, as well as for just houses and units (and either)

    bothGeo <- intersect(intersect(intersect(shGeo, suGeo), rhGeo), ruGeo)
    houseGeo <- intersect(shGeo,rhGeo)
    unitGeo <- intersect(suGeo, ruGeo)
    eitherGeo <- union(houseGeo, unitGeo)

Return a list of acceptable geographies in all four situations.

    return(list(bothGeo = bothGeo,
                houseGeo = houseGeo,
                unitGeo = unitGeo,
                eitherGeo = eitherGeo))  
    }

\  
&nbsp;

### prrApplyThres()

This function creates a variable in the observational data noting whether or not the particular observation is in a geographic area that meets the criteria noted in **prrGeoLimit()**.

This function takes 4 arguments:

1. **thresData**: Output from prrGeoLimit()
2. **transData**: The data frame of the transactions 
3. **timePrefix**: ('YT') A prefix to denote the type of time field used.
4. **geo**: ('postCode') The geographic field used in the **thresData**.

Declare the function

    prrApplyThres <- function(thresData, transData, timePrefix='YT', geo="postCode"){

Create binary vectors indicating whether or not observation meets the 4 different thresholds

    both <- ifelse(transData[,geo] %in% thresData[[1]],1,0)
    house <- ifelse(transData[,geo] %in% thresData[[2]],1,0)
    unit <- ifelse(transData[,geo] %in% thresData[[3]],1,0)
    either <- ifelse(transData[,geo] %in% thresData[[4]],1,0)

Combine 4 vectors into a data frame
    
    all <- as.data.frame(cbind(both, house, unit, either))

Give data frame the proper names (using the **timePrefix**)
  
    names(all) <- paste0(timePrefix, "_", names(all), "_",geo)
  
Return values to 

    return(cbind(transData, all))
    }

\  
&nbsp;

### prrStsGeoWrap()

Wrapper function that applies the **spaceTimeShard()** function across a set of geographies. 

This function takes 7 arguments:

1. **stsData**: Basic transaction data
2. **metrics**: The field(s) to perform the calculation on 
3. **spaceField**: Field indicating the space variable. ('All' if doing entire area)
4. **timeField**: Field indicating the time variable
5. **defDim**: ('Time' or 'Space')  Which dimension is the constant
6. **stsLimit**: How many observations are required in each shard?
7. **calcs**: (list(median='median')) List of calculations to make

                            
Declare the function

    prrStsGeoWrap <- function(stsData, metrics, spaceField, timeField,
                              defDim, stsLimit, calcs){

Source the spaceTimeShard functions
  
    if(!exists('spaceTimeShard')) {
        source(paste0('https://raw.githubusercontent.com/andykrause/',
                    'dataAnalysisTools/master/stShardFunctions.R'))
    }
  
Calculate the sharded values for price

    xPrice <- spaceTimeShard(stsData[stsData$transType == 'sale', ],
                             metric=metrics[1], spaceField=spaceField,
                             timeField=timeField, defDim=defDim, 
                             stsLimit=stsLimit, calcs=calcs)

Calculate the sharded values for rent
  
    xRent <- spaceTimeShard(stsData[stsData$transType == 'rent', ],
                            metric=metrics[2], spaceField=spaceField,
                            timeField=timeField, defDim=defDim, 
                            stsLimit=stsLimit, calcs=calcs)
  
If calculation is to be done on geographic disaggregation then trim the results to only those geographies that meet the criteria.
  
    if(spaceField != 'all'){

Combine names and select acceptable geographies

    okNames <- intersect(names(xPrice[[2]]), names(xRent[[2]]))
    geoPrices <- xPrice[[4]]
    geoPrices <- geoPrices[geoPrices$spaceName %in% okNames,]
    geoRents <- xRent[[4]]
    geoRents <- geoRents[geoRents$spaceName %in% okNames,]

Convert to an exportable table

    geoTable <- data.frame(timeName=geoPrices$timeName,
                           spaceName=geoPrices$spaceName,
                           price=geoPrices$median,
                           rent=geoRents$median,
                           yield=(geoRents$median * 52) / geoPrices$median)

If not splitting by geography then you can go straight to building the table

    } else {
    geoTable <- data.frame(timeName=xPrice$stsDF$timeName,
                           spaceName='all',
                           price=xPrice$stsDF$median,
                           rent=xRent$stsDF$median,
                           yield=((xRent$stsDF$median * 52) / 
                             xPrice$stsDF$median))
    }
    
Return data to function.  Returning a dataframe of the results table as well as the tables of the observation counts by geo and time (These are used later).

    return(list(stsDF=geoTable,
                priceStsTable=xPrice$stTable,
                rentStsTable=xRent$stTable))
    }


\  
&nbsp;

### prrSaleRentMatch()

Function to compare price and rent on matched properties

This function takes 6 arguments:

1. **sales**: Data frame of sales observations
2. **rentals**: Data frame of rental observations 
3. **matchField**: ('ID') Field indicating the variable to match sales and rents
4. **saleField**: ('Price') Field indicating sale price
5. **rentField**: ('Rent') Field indicating rental amount
6. **timeField**: Field indicating the time variable

Declare the function

    prrSaleRentMatch <- function(sales, rentals, matchField = 'ID', saleField = 'Price',
                                 rentField = 'Rent', timeField = 'Year'){

Remove observations missing matching field

    xSales <- subset(sales, !is.na(sales[matchField]))
    xRentals <- subset(rentals, !is.na(rentals[matchField]))

Sort by match field

    xSales <- xSales[order(xSales[,matchField]),]
    xRentals <- xRentals[order(xRentals[,matchField]),]
  
Extract the matching field

    sMatch <- xSales[ ,matchField]
    rMatch <- xRentals[ ,matchField]
  
Perform the cross matching identification

    mSales <- xSales[!is.na(match(sMatch, rMatch)), ]
    mRentals <- xRentals[!is.na(match(rMatch, sMatch)), ]
  
Merge the two matched datasets together

    mTrans <- merge(mSales[, c(matchField, saleField, timeField)],
                    mRentals[, c(matchField, rentField, timeField)],
                    by=matchField)
  
Rename the matched fields

    names(mTrans) <- c(matchField, 'saleValue', 'saleTime', 'rentValue', 'rentTime')
  
Next we make the time adjustments to the values since rents and sales do not occur at the same time.

Create the rent index

    rentTrend <- as.numeric(tapply(mTrans$rentValue, mTrans$rentTime, median))
    rentIndex <- rentTrend / rentTrend[1]

Create the sale index
  
    saleTrend <- as.numeric(tapply(mTrans$saleValue, mTrans$saleTime, median))
    saleIndex <- saleTrend / saleTrend[1]
  
Make adjustments to the rentals

    rentAdj <- (rentIndex[as.numeric(as.factor(mTrans$saleTime))] /
                rentIndex[as.numeric(as.factor(mTrans$rentTime))])
    mTrans$adjRent <- mTrans$rentValue * rentAdj
  
Make the adjustments to the sales
  
    saleAdj <- (saleIndex[as.numeric(as.factor(mTrans$rentTime))] /
                saleIndex[as.numeric(as.factor(mTrans$saleTime))])
    mTrans$adjSale <- mTrans$saleValue * saleAdj
  
Calculate the annual yield values

    mTrans$saleYield <- (mTrans$adjRent * 52) / mTrans$saleValue
    mTrans$rentYield <- (mTrans$rentValue * 52) / mTrans$adjSale
  
Add the location variables (lat, long, property type and 4 geo aggregations) to the matched data.
  
    mTrans$lga <- xSales$lga[match(mTrans$AddressID, xSales$AddressID)]
    mTrans$sla1 <- xSales$sla1[match(mTrans$AddressID, xSales$AddressID)]
    mTrans$suburb <- xSales$suburb[match(mTrans$AddressID, xSales$AddressID)]
    mTrans$postCode <- xSales$postCode[match(mTrans$AddressID, xSales$AddressID)]
    mTrans$latitude <- xSales$Property_Latitude[match(mTrans$AddressID, 
                                                      xSales$AddressID)]
    mTrans$longitude <- xSales$Property_Longitude[match(mTrans$AddressID, 
                                                        xSales$AddressID)]
    mTrans$PropertyType <- xSales$PropertyType[match(mTrans$AddressID, 
                                                     xSales$AddressID)]

Return the data frame of the matched values

    return(mTrans)  
    }  

\  
&nbsp;

### prrAggrGeoData()

Function to aggregate data by geography

This function takes 6 arguments:

1. **geoList**: A list of all sharded results (9 types) at a given geographic level
2. **indexList**: List of price index for all, house-only and unit-only
3. **geoSplit**: (FALSE) Is the geography split into smaller areas (i.e. not all area)

This function uses two subsidiary custom function:

1. **prrAggrMethData()**
2. **prrWeightsUses()**


Declare the function

    prrAggrGeoData <- function(geoList, indexList, geoSplit=FALSE){
  
Build the dataset of all use combined (mixed) results  

    mixData <- prrAggrMethData(mmObj=geoList$mm$all,
                               irObj=geoList$ir$all,
                               dmObj=geoList$dm$all,
                               pIndex=indexList$all,
                               geoSplit=geoSplit)
  
Build the dataset of all use combined and weighted (mixed weighted) results

If working on the entire geographic area then results are the same

    if(!geoSplit){
      mixDataWgt <- mixData
    } 

If working on a small geographic scale then run **prrAggrMethData with the weighting function

    else {
      mixDataWgt <- prrAggrMethData(mmObj=geoList$mm$all,
                                    irObj=geoList$ir$all,
                                    dmObj=geoList$dm$all,
                                    pIndex=indexList$all,
                                    geoSplit=geoSplit,
                                    wgt=TRUE)
    }
  
Separate and build the results for houses-only and units-only

    houseData <- prrAggrMethData(mmObj=geoList$mm$house,
                                 irObj=geoList$ir$house,
                                 dmObj=geoList$dm$house,
                                 pIndex=indexList$house,
                                 geoSplit=geoSplit)
    unitData <- prrAggrMethData(mmObj=geoList$mm$unit,
                                irObj=geoList$ir$unit,
                                dmObj=geoList$dm$unit,
                                pIndex=indexList$unit,
                                geoSplit=geoSplit)
  
Add labels to the results

    houseData$comp$use <- 'House'
    houseData$diff$use <- 'House'
    unitData$comp$use <- 'Unit'
    unitData$diff$use <- 'Unit'
  
Combine all into a single list

    useData <- list(comp=rbind(houseData$comp,
                               unitData$comp),
                    diff=rbind(houseData$diff,
                               unitData$diff))
  
Create results that are use specific (house vs unit) as well as weighted

If working on the entire area, use the **prrWeightuses()** function to apply weights by use

    if(!geoSplit){
       useWgt <- prrWeightUses(houseData, unitData, geoList, 
                             pIndex=indexList$all, 
                             geoSplit=geoSplit) 
    }
  
If working at a smaller geographic scale then calculate separately for houses and units at the given scale.

    else {
      houseDataW <- prrAggrMethData(mmObj=geoList$mm$house,
                                  irObj=geoList$ir$house,
                                  dmObj=geoList$dm$house,
                                  pIndex=indexList$house,
                                  geoSplit=geoSplit,
                                  wgt=TRUE)
      unitDataW <- prrAggrMethData(mmObj=geoList$mm$unit,
                                 irObj=geoList$ir$unit,
                                 dmObj=geoList$dm$unit,
                                 pIndex=indexList$unit,
                                 geoSplit=geoSplit,
                                 wgt=TRUE)
  
Add labels to the results

    houseDataW$comp$use <- 'House'
    houseDataW$diff$use <- 'House'
    unitDataW$comp$use <- 'Unit'
    unitDataW$diff$use <- 'Unit'

Cacluate the use specific weights

    useWgt <- prrWeightUses(houseDataW, unitDataW, geoList, 
                            pIndex=indexList$all, geoSplit=TRUE)  
    }
  
Return four sets of results  

    return(list(mix=mixData,
                mixWgt=mixDataWgt,
                use=useData,
                useWgt=useWgt))
    }

\  
&nbsp;

### prrAggrMethData()

Function to aggregate data by the different methods

This function takes 6 arguments:

1. **mmObj**: Med Meth obj from prrStsGeoWrap
2. **irObj**: Impute Reg obj from spaceTimeShard()
3. **dmObj**: Match obj from spaceTimeShard()
4. **pIndex**: A price index at the time scale
5. **wgt**: (FALSE) Weight based on the nbr of observation in each area/use?
6. **geoSplit**: (FALSE) Is the geography split into smaller areas (i.e. not all area)


This function uses two subsidiary custom function:

1. **prrConvStsTables()**
2. **prrWeightsUses()**

Declare function 

    prrAggrMethData <- function(mmObj, irObj, dmObj, pIndex, wgt=FALSE, geoSplit=FALSE){  
  
Isolate the correct data from each object

If splitting by geography

     if(geoSplit){
        mmDF <- mmObj$stsDF[,c('timeName', 'spaceName', 'yield')]
     } 

If working on the entire geographic area

     else {
       mmDF <- mmObj$stsDF[ ,c('timeName', 'yield')]
       mmDF$spaceName <- 'all'
     }

Extract the imputed and direct match data

    irDF <- irObj$stsDF
    dmDF <- dmObj$stsDF
  
Rename 'yield' field

    names(irDF)[2] <- names(dmDF)[2] <- 'yield'
  
Determine the spatial areas that exist in all objects.  Start by extracting the names of the geographic areas from each of the objects

    mmGeo <- levels(mmDF$spaceName)
    irGeo <- levels(as.factor(irDF$spaceName))
    dmGeo <- levels(as.factor(dmDF$spaceName))

Intersect all three geographies to find the common set

    allGeo <- intersect(intersect(mmGeo, irGeo),dmGeo)
  
Limit each set to only those with the common set of geographies

    mmDF <- subset(mmDF, mmDF$spaceName %in% allGeo)
    irDF <- subset(irDF, irDF$spaceName %in% allGeo)
    dmDF <- subset(dmDF, dmDF$spaceName %in% allGeo)
  
Extract the number of geo areas and the number of time periods

    oLng <- nrow(mmDF)
    tLng <- length(unique(mmDF$timeName))
  
Build the comparison data set
  
If not weighting, combine all three DFs together then add a method label

   if(!wgt){
    
    comData <- rbind(mmDF, irDF, dmDF)
    comData$method <- c(rep('Median', oLng), rep('Impute', oLng),
                        rep('Match', oLng))
    
  } 
  
If weighting  
  
  else {
    
Extract tables showing counts of each geography

    mmPrice <- mmObj$priceStsTable[rownames(mmObj$priceStsTable) %in% allGeo,]
    mmRent <- mmObj$rentStsTable[rownames(mmObj$rentStsTable) %in% allGeo,]
    
Generate the weights with the custom function

    mmWgts <- prrConvStsTables(mmPrice + mmRent, allGeo)$wgts
    irWgts <- prrConvStsTables(irObj$stTable, allGeo)$wgts
    dmWgts <- prrConvStsTables(dmObj$stTable, allGeo)$wgts
    
Add the individual weights to the data frames of the three methods

    mmDF$wgt <- mmWgts[match(mmDF$spaceName, names(mmWgts))]  
    irDF$wgt <- irWgts[match(irDF$spaceName, names(irWgts))]  
    dmDF$wgt <- dmWgts[match(dmDF$spaceName, names(dmWgts))]  
    
Calculate the weighted yield components

    mmDF$wYield <- mmDF$yield * mmDF$wgt
    irDF$wYield <- irDF$yield * irDF$wgt
    dmDF$wYield <- dmDF$yield * dmDF$wgt
    
Sum to get the weighted total

    mmYields <- tapply(mmDF$wYield, mmDF$timeName, sum)
    irYields <- tapply(irDF$wYield, irDF$timeName, sum)
    dmYields <- tapply(dmDF$wYield, dmDF$timeName, sum)
    
Combine all three together, including the time labels and the space labels to make the comparison datasets.

    comData <- data.frame(timeName=rep(1:tLng, 3),
                          spaceName=rep('all', 3*tLng),
                          yield = c(mmYields, irYields, dmYields),
                          method=c(rep('Median', tLng), rep('Impute', tLng),
                                   rep('Match', tLng)))
    

Create the data for analyzing the differences between the three methods.  Begin by extract each from the comparison data and then setting the length to equal the number of time periods.

    mmDF <- comData[comData$method == 'Median', ]
    irDF <- comData[comData$method == 'Impute', ]
    dmDF <- comData[comData$method == 'Match', ]
    oLng <- tLng
    }
  
Re-order the factor levels for better plotting and analysis later

    comData$method <- factor(comData$method,
                             levels=c('Median', 'Impute', 'Match'))
  
Create the data for analyzing the differences between the three methods.  Bind all three methods back together and add on the price index (**pIndex**), the methods and differences between the results.

    difData <- rbind(mmDF, irDF, dmDF)
    difData$pIndex <- rep(pIndex, 3)
    difData$yield <- NULL
    difData$method <- c(rep('Impute - Median', oLng),
                        rep('Match - Median', oLng),
                        rep('Match - Impute', oLng))
    difData$dif <- c(irDF$yield - mmDF$yield,
                     dmDF$yield - mmDF$yield,
                     dmDF$yield - irDF$yield)
  
Re-order the factor levels for better plotting and analysis later

    difData$method <- factor(difData$method,
                             levels=c('Impute - Median', 'Match - Median',
                                      'Match - Impute'))
  
Return both the comparison and the differencing data

    return(list(comp = comData,
                diff = difData))
    }

\  
&nbsp;

### prrConvStsTables()

Function that calculates locations count weights

This function takes 6 arguments:

1. **stsTable**: stsTable object from spaceTimeShard() function
2. **allGeo**: List of Geographies that meet the criteria

Declare function 

    prrConvStsTables <- function(stTable, allGeo){

Ensure that table is a data frame format
  
    stTable <- as.data.frame(stTable)
  
Remove geographies that do not meet criteria

    stTable <- stTable[rownames(stTable) %in% allGeo, ]

Add up the row sums
  
    stSums <- rowSums(stTable)
  
Create weights based on the row sums  
  
    stWgts <- stSums / sum(stSums)
  
Return a list containing the row sums and the weights  
  
    return(list(sums=stSums,
                wgts=stWgts))
    }
    
\  
&nbsp;

### prrWeightUses()

Function that combines and weights house and unit results 

This function takes 5 arguments:

1. **hDataW**: House wgt data from prrAggrMethData()
2. **uDataW**: Unit wgt data from prrAggrMethData
3. **geoList**: Full geolist from  prrAggrGeoData
4. **pIndex**: Price time index generated from impute regressions
5. **geoSplit**: (FALSE) ArestsTable object from spaceTimeShard() function

Declare function 

prrWeightUses <- function(hDataW, uDataW, geoList, pIndex, geoSplit=FALSE){  
  
Operation differ based on whether or not we are doing a global or disaggregated analysis

If doing disaggregated

    if(geoSplit){
    
Begin by extracting the lists of accepting geographies for house and unit, for sales and rents for all three methods.

    mmHPrice <- geoList$mm$house$priceStsTable
    mmHRent <- geoList$mm$house$rentStsTable
    mmUPrice <- geoList$mm$unit$priceStsTable
    mmURent <- geoList$mm$unit$rentStsTable
    irHTable <- geoList$ir$house$stTable
    irUTable <- geoList$ir$unit$stTable
    dmHTable <- geoList$dm$house$stTable
    dmUTable <- geoList$dm$unit$stTable
  
Combine all geography names into a single vector.  Remove duplicates, create a table and then create an object with the counts of each geography (**geoTable**)

    allGeos <- c(rownames(mmHPrice), rownames(mmHRent), rownames(mmUPrice), 
                rownames(mmURent), rownames(irHTable), rownames(irUTable),
                rownames(dmHTable), rownames(dmUTable))
    geoNames <- names(table(allGeos))
    geoTable <- as.numeric(table(allGeos))
    
Choose those geographies that are present in all situations

    allGeos <- geoNames[which(geoTable == 8)]

Limit the list of original geographies to those present everywhere

    mmHPrice <- mmHPrice[rownames(mmHPrice) %in% allGeos, ]
    mmHRent <- mmHRent[rownames(mmHRent) %in% allGeos, ]
    mmUPrice <- mmUPrice[rownames(mmUPrice) %in% allGeos, ]
    mmURent <- mmURent[rownames(mmURent) %in% allGeos, ]
    irHTable <- irHTable[rownames(irHTable) %in% allGeos, ]
    irUTable <- irUTable[rownames(irUTable) %in% allGeos, ]
    dmHTable <- dmHTable[rownames(dmHTable) %in% allGeos, ]
    dmUTable <- dmUTable[rownames(dmUTable) %in% allGeos, ]
    }

If doing a global analysis instead

    else {

Extract all data from price/rent, house/unit and all three methods    

    mmHPrice <- geoList$mm$house$priceStsTable
    mmHRent <- geoList$mm$house$rentStsTable
    mmUPrice <- geoList$mm$unit$priceStsTable
    mmURent <- geoList$mm$unit$rentStsTable
    irHTable <- geoList$ir$house$stTable
    irUTable <- geoList$ir$unit$stTable
    dmHTable <- geoList$dm$house$stTable
    dmUTable <- geoList$dm$unit$stTable
    }
  
Calculate weights for the median method
  
    mmHwgt <- (sum(mmHPrice + mmHRent)) / (sum(mmHPrice + mmHRent) +
                                             sum(mmUPrice + mmURent))
    mmUwgt <- 1-mmHwgt
  
Calculate weights for the impute method
  
    irHwgt <- sum(irHTable) / (sum(irHTable + irUTable))  
    irUwgt <- 1-irHwgt
  
Calculate weights for the match method

    dmHwgt <- sum(dmHTable) / (sum(dmHTable + dmUTable))  
    dmUwgt <- 1-dmHwgt
  
Combine weights into house and unit specific weight tables

    hWgts <- c(rep(mmHwgt, 20), rep(irHwgt, 20), rep(dmHwgt, 20))
    uWgts <- c(rep(mmUwgt, 20), rep(irUwgt, 20), rep(dmUwgt, 20))
  
Build the comparison data.  First create a similar object to the house datat (**hDataW**).  This is just a filler object.  Then update the yield values in this object to be the weighted yields for the sum of houses and units.  Remove the use field
  
    compData <- hDataW$comp
    compData$yield <- ((hDataW$comp$yield * hWgts) + 
                         (uDataW$comp$yield * uWgts))
    compData$use <- NULL
  
Re order the factor levels for easier plotting and analysis later

    compData$method <- factor(compData$method,
                            levels=c('Median', 'Impute', 'Match'))

Create the differencing data (data to compare method differences to appreciation rates)

Build initial objects and lenght of time counts

    difData <- compData
    oLng <- 20
  
Add the price index to the difference data object and remove the yield field  
  
    difData$pIndex <- rep(pIndex, 3)
    difData$yield <- NULL
  
Add the method labels

    difData$method <- c(rep('Impute - Median', oLng),
                        rep('Match - Median', oLng),
                        rep('Match - Impute', oLng))

Split into three data frames based on method type

    mmDF <- subset(compData, method=='Median')
    irDF <- subset(compData, method=='Impute')
    dmDF <- subset(compData, method=='Match')
  
Take the yields from these splits and add back onto the difference data frame

    difData$dif <- c(irDF$yield - mmDF$yield,
                     dmDF$yield - mmDF$yield,
                     dmDF$yield - irDF$yield)
  
Re order the factor levels for easier plotting and analysis later

    difData$method <- factor(difData$method,
                             levels=c('Impute - Median', 'Match - Median',
                                    'Match - Impute'))
  
Return a list of the comparison data and the difference data

    return(list(comp=compData,
                diff=difData))    
    }
    
\  
&nbsp;

### prrGetYields()

Function that combines and weights house and unit results 

This function takes 1 argument:

1. **prrObj**: Data object from **prrAggrMethData()**

Declare function     
    
Extract the raw yields from a **prrObj** 

    prrGetYields <- function(prrObj){

Extract the mixed (house and unit) yields

    mixYields <- prrObj$mix$comp
  
Convert these yields into a list, separated by method

    mix <- list(median=subset(mixYields, method=='Median'),
                impute=subset(mixYields, method=='Impute'),
                match=subset(mixYields, method=='Match'))
  

Next we extract and build a list of house-only yields

    houseYields <- subset(prrObj$use$comp, use=='House')
    house <- list(median=subset(houseYields, method=='Median'),
                  impute=subset(houseYields, method=='Impute'),
                  match=subset(houseYields, method=='Match'))
  
Then we extract and build a list of unit-only yields
  
    unitYields <- subset(prrObj$use$comp, use=='Unit')
    unit <- list(median=subset(unitYields, method=='Median'),
                 impute=subset(unitYields, method=='Impute'),
                 match=subset(unitYields, method=='Match'))
  
Return a list of the mixed, house and unit results

    return(list(mix=mix,
                house=house,
                unit=unit))
    }    
    
\  
&nbsp;

### prrPredModelWrap()

Wrapper function to calculate the predictive accuracy of various yield trends 

This function takes 5 arguments:

1. **dmData**: Dataset of direct matches on houses and units
2. **yieldData**: Yield data object from **prrGetYields()**
3. **byUse**: (FALSE) Separate by use?
4. **byGeog**: (FALSE) Separate by Geography
5. **geoField**: (NULL) If byGeog, then which field has the geographic labels

This function uses one subsidiary custom function:

1. **prrErrorByMethod()**

Declare Function

    prrPredModelWrap <- function(dmData, yieldData, byUse=FALSE, byGeog=FALSE, geoField=NULL){
  
Create a small helper function to count number of observations

    geoCount <- function(x){nrow(rbind.fill(x))}
  
Throw an error and stop function if no geoField is specified and one is required (byGeog=TRUE)

    if(byGeog & is.null(geoField)) return(cat('Must specify geoField'))
  
If calculating differences by use

  if(byUse){
    
Begin by subsetting data into houses and units

    hData <- subset(dmData, PropertyType == 'House')
    uData <- subset(dmData, PropertyType == 'Unit')
    
If then calculating by geography

    if(byGeog){
    
Start by working on houses

Get geography names and extract necessary data

      geoListH <- levels(yieldData$house$median$spaceName)
      geoDataH <- lapply(geoListH, prrExtractGeoData, xData=hData, 
                         geoField=geoField)
      
Identify non-empty datasets (by geog)

      hCount <- unlist(lapply(geoDataH, nrow))
      idH <- which(hCount > 0)
      
Extract necessary geographic yield information

      geoYieldsH <- lapply(geoListH, prrExtractGeoYields, yieldData=yieldData$house)
      
Identify remaining non-empty dataset (by geog)

      idYH <- lapply(geoYieldsH, geoCount)
      idYH <- which(idYH > 0)
      
Select those that meet both criteria

      idH <- intersect(idH, idYH)
      
Trim data to those that are not empty

      geoDataH <- geoDataH[idH]
      geoYieldsH <- geoYieldsH[idH]
      
Estimate the prediction error using the **PrrErrorByMethod** custom function

      geoH <- mapply(prrErrorByMethod, mData=geoDataH, yieldData=geoYieldsH)
      
Combine and rename results
      hResults <- rbind.fill(geoH)
      hResults$use <- 'house'
      hResults$geog <- geoField
      
Now, create the same analysis for for units  
      
Get geography names and extract necessary data

      geoListU <- levels(yieldData$unit$median$spaceName)
      geoDataU <- lapply(geoListU, prrExtractGeoData, xData=uData, 
                         geoField=geoField)
      
Identify non-empty datasets (by geog)

      uCount <- unlist(lapply(geoDataU, nrow))
      idU <- which(uCount > 0)
      
Extract necessary geographic yield information
      geoYieldsU <- lapply(geoListU, prrExtractGeoYields, yieldData=yieldData$unit)
      
Identify remaining non-empty dataset (by geog)
      
      idYU <- lapply(geoYieldsU, geoCount)
      idYU <- which(idYU > 0)
      
Select those that meet both criteria

      idU <- intersect(idU, idYU)
      
Trim data to those that are not empty

      geoDataU <- geoDataU[idU]
      geoYieldsU <- geoYieldsU[idU]
      
Estimate the prediction error  

      geoU <- mapply(prrErrorByMethod, mData=geoDataU, yieldData=geoYieldsU)
      
Combine and rename results
      uResults <- rbind.fill(geoU)
      uResults$use <- 'unit'
      uResults$geog <- geoField
      
Merge house and unit results
      
      xResults <- rbind(hResults, uResults)
      } 
      
If NOT calculating the results by geography      

      else {
      
Calculate errors for houses

      hResults <- prrErrorByMethod(hData, yieldData$house)
      hResults <- rbind.fill(hResults)
      hResults$use <- 'house'
      
Calculate errors for units

      uResults <- prrErrorByMethod(uData, yieldData$unit)
      uResults <- rbind.fill(uResults)
      uResults$use <- 'unit'
      
Combine results

       xResults <- rbind(hResults, uResults)
       xResults$geog <- 'all'
       }
      } 
  
If NOT calculating by use  
  
      else {
    
But by geography
    
      if(byGeog){
      
Extract relevant geography names

      geoList <- levels(yieldData$mix$median$spaceName)
      
 Extract geographic base data

      geoData <- lapply(geoList, prrExtractGeoData, xData=dmData, 
                        geoField=geoField)
      
Extract yield trends

      geoYields <- lapply(geoList, prrExtractGeoYields, yieldData=yieldData$mix)
      
Calculate all errors

      geo <- mapply(prrErrorByMethod, mData=geoData, yieldData=geoYields)
      xResults <- rbind.fill(geo)
      xResults$geog <- geoField
      } 

If not by use and not by Geography
      
      else {

Calculate error results

      xResults <- prrErrorByMethod(dmData, yieldData$mix)
      xResults <- rbind.fill(xResults)
      xResults$geog <- 'all'
    }

Add use label

    xResults$use <- 'mix'
    }
  
Return values as a list, with a separate list object for each method

    return(list(median=subset(xResults, method=='median'),
                impute=subset(xResults, method=='impute'),
                match=subset(xResults, method=='match')))  
    }

\  
&nbsp;

### prrExtractGeoData()

Helper function to extract geographic data from a given dataset

This function takes 3 arguments:

1. **geoName**: The specific geographic name to extract
2. **xData**: The full transaction dataset
3. **geoField**: Field containing the geographic labels

Declare Function

    prrExtractGeoData <- function(geoName, xData, geoField){
  
ID and extract field

    gData <- xData[ ,geoField]
  
Label matching rows

    idx <- which(gData == geoName)
  
Return rows containing the matching geography 

    return(xData[idx, ])
    }

\  
&nbsp;

### prrExtractGeoYields()

Helper function to extract the three yield types for a given geography 

This function takes 2 arguments:

1. **geoName**: The specific geographic name to extract
2. **yieldData**: Yield data from prrGetYields()$mix

Declare function

    prrExtractGeoYields <- function(geoName, yieldData){
  
Extract yields for each method

    gMedian <- subset(yieldData$median, spaceName==geoName)
    gImpute <- subset(yieldData$impute, spaceName==geoName)
    gMatch <- subset(yieldData$match, spaceName==geoName)
  
Return values a list of each method  
  
    return(list(median=gMedian,
                impute=gImpute,
                match=gMatch))
    }

\  
&nbsp;

### prrErrorByMethod()

Wrapper function to spread the error calcs over all three methods

This function takes 2 arguments:

1. **geoName**: The matched dataset
2. **yieldData**: Yield data from prrGetYields()$mix

This function uses one subsidiary custom function:

1. **prrCalcPredError()**

Declare function

prrErrorByMethod <- function(mData, yieldData){
  
Calculate errors for median method
  
    medianError <- prrCalcPredError(mData, yieldData$median)
    medianError$method='median'
  
Calculate error for imputation method
  
   imputeError <- prrCalcPredError(mData, yieldData$impute)
   imputeError$method='impute'
  
Calculate error for matching method
  
    
    matchError <- prrCalcPredError(mData, yieldData$match)
    matchError$method='match'
  
Return values as a list of all three methods

    return(list(median=medianError,
              impute=imputeError,
              match=matchError))
  
}

\  
&nbsp;

### prrCalcPredError()

Basic predictive error calculating engine

This function takes 2 arguments:

1. **mData**: Dataset of matched sales and rentals 
2. **yData**: Timeseries of yield estimates

Declare function

    prrCalcPredError <- function(mData, yData){
  
Subset into sales and rentals based on which observation is first

    sData <- subset(mData, saleTime <= rentTime)
    rData <- subset(mData, saleTime > rentTime)
    sData$tType <- 'sale'
    rData$tType <- 'rent'
  
Add the yield information at the time of the first transaction

    sData$pYield <- yData$yield[match(sData$rentTime, yData$timeName)]  
    rData$pYield <- yData$yield[match(rData$saleTime, yData$timeName)]  
  
Predict rental value of sales and the error

    sData$pValue <- ((sData$adjSale * sData$pYield) / 52) 
    sData$error <- (sData$rentValue - sData$pValue) / sData$rentValue

Precict sale value of rentals and the error    

    rData$pValue <- (rData$adjRent * 52) / rData$pYield
    rData$error <- (rData$saleValue - rData$pValue) / rData$saleValue

Merge data together  

    xData <- rbind(sData[,c('uID', 'tType', 'error')],
                   rData[,c('uID', 'tType', 'error')])
  
Return values

    return(xData)
    }

------------------------------------------------------------------------------------------
